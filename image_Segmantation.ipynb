{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SydanJainen/Cell-image-segmentation/blob/main/image_Segmantation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUnkC-z2j3sP"
      },
      "source": [
        "# CELL IMAGE SEGMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGnmA8UWy5tw"
      },
      "source": [
        "## HYPERPARAMETER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xEiCoWKNjyc5"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNEL = 3\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 100\n",
        "PADDING = 10\n",
        "STRIDE = 2\n",
        "PATIENCE = 5\n",
        "\n",
        "VAL_SPLIT = 0.30\n",
        "SEED = 42\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "EPS = 1e-7\n",
        "\n",
        "width_out = 256\n",
        "height_out = 256\n",
        "\n",
        "STEP_AUGMENTATION = 15\n",
        "\n",
        "NAME = 'unet'\n",
        "\n",
        "LossName = 'BCEDiceLoss'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbSiAvaAlH6i"
      },
      "source": [
        "## CHECK IF COLAB OR OFFLINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T_aSkDT7lM5u"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  isColab = True\n",
        "else:\n",
        "  isColab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9csuVQSleyd"
      },
      "source": [
        "## CHECK IF GPU IS AVAILABLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb4Ry-c0lhc4",
        "outputId": "e90183e9-dda3-45d4-ff0f-1bde3f05743c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 GPU available\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "num_gpu = torch.cuda.device_count()\n",
        "print(f'{num_gpu} GPU available')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7_1mjbClJpv"
      },
      "source": [
        "## REQUIREMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r4T96G8lGmp",
        "outputId": "2e272334-3a4f-4114-fdd2-96e455e7c9b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Check if folder is correct\n"
          ]
        }
      ],
      "source": [
        "if isColab:\n",
        "  import os\n",
        "  from google.colab import files, drive\n",
        "  if (not 'kaggle.json' in os.listdir()):\n",
        "    print(\"Upload kaggle.json\")\n",
        "    files.upload()\n",
        "    !mkdir ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "  drive.mount('/content/drive')\n",
        "  DB_PATH = '/content/drive/MyDrive/scuola/deeplife/'\n",
        "else:\n",
        "  print(\"Check if folder is correct\")\n",
        "  DB_PATH = 'database/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3gVoRZVkj1i"
      },
      "source": [
        "## IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "if isColab:\n",
        "    !pip install opencv-python\n",
        "    !pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7uOPomQDkoSl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import tqdm\n",
        "import json\n",
        "import gc\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "from cv2 import imwrite\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# aplly SEED to all random functions\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "dateTime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "#create a fodler with the hyperparameters name to save the results\n",
        "if not os.path.exists(f'results'):\n",
        "    os.makedirs(f'results')\n",
        "if not os.path.exists(f'results{NAME}/{NAME}_{EPOCHS}epochs_{BATCH_SIZE}batch_{PATIENCE}patience_{STEP_AUGMENTATION}augm'):\n",
        "    os.makedirs(f'results{NAME}/{NAME}_{EPOCHS}epochs_{BATCH_SIZE}batch_{PATIENCE}patience_{STEP_AUGMENTATION}augm')\n",
        "RESULTS_PATH = f'results{NAME}/{NAME}_{EPOCHS}epochs_{BATCH_SIZE}batch_{PATIENCE}patience_{STEP_AUGMENTATION}augm/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a9s0UE9y5t2",
        "outputId": "edff37fe-410b-4e45-a559-276d311aa79e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database folder found\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(DB_PATH):\n",
        "    print(\"Database folder found\")\n",
        "else:\n",
        "    print(\" Generate the database\")\n",
        "    os.makedirs(DB_PATH)\n",
        "    os.chdir(DB_PATH)\n",
        "    if(isColab):\n",
        "        !pip install kaggle\n",
        "        !kaggle competitions download -c data-science-bowl-2018\n",
        "        !unzip -q data-science-bowl-2018.zip -d ./\n",
        "        !unzip -q cellular-segmentation/stage1_train.zip -d unpreocessed/\n",
        "        if os.path.exists('./cellular-segmentation') and os.path.exists('./data-science-bowl-2018.zip'):\n",
        "            !rm data-science-bowl-2018.zip\n",
        "    else:\n",
        "        print(\"Download the database and place it in the folder\")\n",
        "    os.chdir('..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6tChQpKompII"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = os.path.join(DB_PATH, \"train\")\n",
        "LABEL_PATH = os.path.join(DB_PATH, \"label\")\n",
        "\n",
        "if STEP_AUGMENTATION > 0:\n",
        "    AUG_IMAGE_PATH = os.path.join(DB_PATH, \"augmented/train\")\n",
        "    AUG_LABEL_PATH = os.path.join(DB_PATH, \"augmented/label\")\n",
        "\n",
        "# Ensure both directories have the same number of files\n",
        "number_images = len(os.listdir(IMAGE_PATH))\n",
        "number_labels = len(os.listdir(LABEL_PATH))\n",
        "\n",
        "if STEP_AUGMENTATION > 0:\n",
        "    number_augmented_images = len(os.listdir(AUG_IMAGE_PATH))\n",
        "    number_augmented_labels = len(os.listdir(AUG_LABEL_PATH))\n",
        "\n",
        "assert number_images == number_labels\n",
        "\n",
        "if STEP_AUGMENTATION > 0:\n",
        "    assert number_augmented_images == number_augmented_labels\n",
        "\n",
        "# LOAD PIL IMAGES AND LABELS\n",
        "image_files = sorted(os.listdir(IMAGE_PATH))\n",
        "label_files = sorted(os.listdir(LABEL_PATH))\n",
        "\n",
        "if STEP_AUGMENTATION > 0:\n",
        "    aug_image_files = sorted(os.listdir(AUG_IMAGE_PATH))\n",
        "    aug_label_files = sorted(os.listdir(AUG_LABEL_PATH))\n",
        "\n",
        "# Split into train and temp sets\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(image_files, label_files, test_size=VAL_SPLIT, random_state=SEED)\n",
        "\n",
        "if STEP_AUGMENTATION > 0:\n",
        "    train_aug_images, val_aug_images, train_aug_labels, val_aug_labels = train_test_split(aug_image_files, aug_label_files, test_size=VAL_SPLIT, random_state=SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting and resizing train images and masks ... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 670/670 [00:05<00:00, 125.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Get and resize train images and masks\n",
        "x_train = np.zeros((number_images, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL), dtype=np.uint8)\n",
        "y_train = np.zeros((number_labels, height_out, width_out, 1))\n",
        "print('Getting and resizing train images and masks ... ')\n",
        "sys.stdout.flush()\n",
        "for length in tqdm.tqdm(range(number_images)):\n",
        "    img = cv2.imread(IMAGE_PATH + '/' + image_files[length])\n",
        "    x_train[length] = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    mask = cv2.imread(LABEL_PATH + '/' + label_files[length], cv2.IMREAD_GRAYSCALE)\n",
        "    y_train[length] = cv2.resize(mask, (width_out, height_out), interpolation=cv2.INTER_AREA).reshape(height_out, width_out, 1)\n",
        "\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 9731/10050 [01:15<00:02, 129.06it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m y_train_aug \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((number_augmented_labels, height_out, width_out, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m length \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(number_augmented_images)):\n\u001b[1;32m----> 5\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAUG_IMAGE_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maug_image_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     x_train_aug[length] \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (IMG_WIDTH, IMG_HEIGHT), interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_AREA)\n\u001b[0;32m      8\u001b[0m     mask \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(AUG_LABEL_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m aug_label_files[length], cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if STEP_AUGMENTATION > 0:\n",
        "    x_train_aug = np.zeros((number_augmented_images, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL), dtype=np.uint8)\n",
        "    y_train_aug = np.zeros((number_augmented_labels, height_out, width_out, 1))\n",
        "    for length in tqdm.tqdm(range(number_augmented_images)):\n",
        "        img = cv2.imread(AUG_IMAGE_PATH + '/' + aug_image_files[length])\n",
        "        x_train_aug[length] = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        mask = cv2.imread(AUG_LABEL_PATH + '/' + aug_label_files[length], cv2.IMREAD_GRAYSCALE)\n",
        "        y_train_aug[length] = cv2.resize(mask, (width_out, height_out), interpolation=cv2.INTER_AREA).reshape(height_out, width_out, 1)\n",
        "    print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# draw random sample from the dataset\n",
        "\n",
        "def draw_sample(x, y):\n",
        "    idx = random.randint(0, len(x)-1)\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    ax[0].imshow(x[idx])\n",
        "    ax[0].set_title('Image')\n",
        "    ax[0].axis('off')\n",
        "    ax[1].imshow(y[idx].squeeze(), cmap='gray')\n",
        "    ax[1].set_title('Mask')\n",
        "    ax[1].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "draw_sample(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(20, 16))\n",
        "\n",
        "x, y = 4, 4\n",
        "for i in range(y):\n",
        "    index = random.randint(0, len(x_train))\n",
        "    plt.subplot(y, x, i * x + 1)\n",
        "    plt.imshow(x_train[index])\n",
        "    plt.title('Image')\n",
        "\n",
        "    plt.subplot(y, x, i * x + 2)\n",
        "    plt.imshow(y_train[index].squeeze(), cmap='gray')\n",
        "    plt.title('Mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "class Nuc_Seg(Dataset):\n",
        "    def __init__(self, images_np, masks_np):\n",
        "        self.images_np = images_np\n",
        "        self.masks_np = masks_np\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images_np)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image_np = self.images_np[idx]\n",
        "        mask_np = self.masks_np[idx]\n",
        "        # Convert numpy arrays to tensors\n",
        "        image = TF.to_tensor(image_np)\n",
        "        mask = TF.to_tensor(mask_np.astype(np.int32))\n",
        "\n",
        "        image = TF.resize(image, (256, 256))\n",
        "        mask = TF.resize(mask, (256, 256))\n",
        "        \n",
        "        return image, mask\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state = SEED)\n",
        "\n",
        "if STEP_AUGMENTATION > 0:\n",
        "    X_train_aug, X_val_aug, Y_train_aug, Y_val_aug = train_test_split(x_train_aug, y_train_aug, test_size = 0.1, random_state = SEED)\n",
        "\n",
        "if STEP_AUGMENTATION > 0:\n",
        "    X_train = np.concatenate((X_train, X_train_aug), axis = 0)\n",
        "    Y_train = np.concatenate((Y_train, Y_train_aug), axis = 0)\n",
        "    X_val = np.concatenate((X_val, X_val_aug), axis = 0)\n",
        "    Y_val = np.concatenate((Y_val, Y_val_aug), axis = 0)\n",
        "\n",
        "train_dataset = Nuc_Seg(X_train, Y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
        "val_dataset = Nuc_Seg(X_val, Y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "def plot_sample(dataset, idx):\n",
        "    \"\"\"\n",
        "    Plots a sample from the dataset.\n",
        "    \n",
        "    Args:\n",
        "        dataset (Dataset): The dataset from which to plot a sample.\n",
        "        idx (int): The index of the sample to plot.\n",
        "    \"\"\"\n",
        "    image, mask = dataset[idx]\n",
        "\n",
        "    # Convert tensors to PIL images for plotting\n",
        "    image = TF.to_pil_image(image)\n",
        "    mask = TF.to_pil_image(mask)\n",
        "\n",
        "    # Plot the image and mask\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title('Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(mask, cmap='gray')\n",
        "    axes[1].set_title('Mask')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_sample(train_dataset, idx=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FtlKMGZ3FTU"
      },
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM_FQJRV3GXj"
      },
      "outputs": [],
      "source": [
        "class UNETV1(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
        "    ):\n",
        "        super(UNETV1, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UNetPlusPlus(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNetPlusPlus, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part\n",
        "        self.down_conv = nn.ModuleList()\n",
        "        for feature in features:\n",
        "            self.down_conv.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part\n",
        "        self.up_conv = nn.ModuleList()\n",
        "        for feature in reversed(features):\n",
        "            self.up_conv.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
        "            self.up_conv.append(DoubleConv(feature * 2, feature))\n",
        "\n",
        "        self.nested_conv = nn.ModuleList()\n",
        "        for feature in features:\n",
        "            self.nested_conv.append(nn.ModuleList([DoubleConv(feature * (i + 2), feature) for i in range(len(features))]))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "        nested_connections = []\n",
        "\n",
        "        # Downward path\n",
        "        for down in self.down_conv:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        # Upward path\n",
        "        for idx in range(len(self.up_conv) // 2):\n",
        "            skip_connection = skip_connections[idx]\n",
        "\n",
        "            x = self.up_conv[2 * idx](x)\n",
        "\n",
        "            nested_concat = torch.cat([x, skip_connection], dim=1)\n",
        "            nested_conv = self.nested_conv[len(self.up_conv) // 2 - idx - 1][idx]\n",
        "\n",
        "            if idx > 0:\n",
        "                for j in range(idx):\n",
        "                    nested_concat = torch.cat([nested_concat, nested_connections[-(idx - j)]], dim=1)\n",
        "                nested_conv = self.nested_conv[len(self.up_conv) // 2 - idx - 1][idx - 1]\n",
        "\n",
        "            x = nested_conv(nested_concat)\n",
        "            nested_connections.append(x)\n",
        "\n",
        "            x = self.up_conv[2 * idx + 1](x)\n",
        "\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        return x * psi\n",
        "\n",
        "class AttentionUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(AttentionUNet, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.attentions = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        # Attention blocks\n",
        "        for feature in reversed(features):\n",
        "            self.attentions.append(AttentionBlock(feature, feature, feature//2))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            # Apply attention\n",
        "            skip_connection = self.attentions[idx//2](g=x, x=skip_connection)\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UNET(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
        "    ):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WRJPsuHnhTI"
      },
      "source": [
        "METRICS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq9xXGOupSTS"
      },
      "source": [
        "## Jaccard Index: Measuring Intersection Over Union (IoU)\n",
        "\n",
        "The **Jaccard Index** is a metric used to compare the similarity between two sets. It calculates the ratio of the size of the intersection of the sets to the size of their union. Mathematically, it's defined as:\n",
        "\n",
        "$$\n",
        "J(A, B) = |A ∩ B| / |A ∪ B|\n",
        "$$\n",
        "\n",
        "## Jaccard Loss \n",
        "\n",
        "The **Jaccard Loss** is derived from the Jaccard index and serves as a loss function during model training. This loss function is particularly useful for image segmentation tasks where the goal is to optimize the model for accurate segmentation. It's defined as:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{Jaccard}}(\\mathbf{p}, \\mathbf{t}) = 1 - J(\\mathbf{p}, \\mathbf{t})\n",
        "$$\n",
        "\n",
        "where $J(\\mathbf{p}, \\mathbf{t})$ is the Jaccard index computed between the predicted and true values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEEGWzztni9P"
      },
      "outputs": [],
      "source": [
        "def jaccard(preds, trues, is_average=True):\n",
        "    num = preds.size(0)\n",
        "    \n",
        "    preds = preds.contiguous()\n",
        "    trues = trues.contiguous()\n",
        "\n",
        "    preds = (preds > 0.5).float()\n",
        "    trues = (trues > 0.5).float()\n",
        "\n",
        "    intersection = (preds * trues).sum(1)\n",
        "    scores = (intersection + EPS) / ((preds + trues).sum(1) - intersection + EPS)\n",
        "\n",
        "    score = scores.sum()\n",
        "    if is_average:\n",
        "        score /= num\n",
        "    return torch.clamp(score, 0., 1.)\n",
        "\n",
        "def jaccard_round(preds, trues, is_average=True):\n",
        "    preds = torch.round(preds)\n",
        "    return jaccard(preds, trues, is_average=is_average)\n",
        "\n",
        "\n",
        "class JaccardLoss(nn.Module):\n",
        "    def __init__(self, size_average=True):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return jaccard(input, target, self.size_average)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dice Loss\n",
        "\n",
        "The Dice Loss is a metric used to evaluate the performance of models in tasks like image segmentation. It focuses on the overlap between the predicted segmentation mask and the ground truth mask, penalizing models that incorrectly classify pixels.\n",
        "\n",
        "A: Predicted segmentation mask pixels\n",
        "\n",
        "B: Ground truth segmentation mask pixels\n",
        "\n",
        "$$\n",
        "L_Dice(A, B) = 1 - 2 * |A ∩ B| / (|A| + |B|)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GoxOigbn0l9"
      },
      "outputs": [],
      "source": [
        "def dice_loss(preds, trues, smooth=1e-6, is_average=True):\n",
        "    preds = preds.contiguous()\n",
        "    trues = trues.contiguous()\n",
        "\n",
        "    preds = (preds > 0.5).float()\n",
        "    trues = (trues > 0.5).float()\n",
        "    \n",
        "    intersection = (preds * trues).sum(dim=(1, 2, 3))\n",
        "    scores = (2. * intersection + smooth) / (preds.sum(dim=(1, 2, 3)) + trues.sum(dim=(1, 2, 3)) + smooth)\n",
        "    \n",
        "    if is_average:\n",
        "        return 1 - scores.mean()\n",
        "    else:\n",
        "        return 1 - scores\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if input.max() > 1:\n",
        "            input = torch.sigmoid(input)\n",
        "        return dice_loss(input, target, smooth=1e-6, is_average=self.size_average)\n",
        "\n",
        "class BCEDiceLoss(nn.Module):\n",
        "    def __init__(self, bce_weight=0.5, smooth=1e-6):\n",
        "        super(BCEDiceLoss, self).__init__()\n",
        "        self.bce_weight = bce_weight\n",
        "        self.dice_loss = DiceLoss()\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        # Ensure targets are float\n",
        "        if targets.max() > 1:\n",
        "            targets = targets/255\n",
        "        targets = targets.float()\n",
        "        targets = (targets > 0.5).float()\n",
        "        bce = self.bce_loss(preds, targets)\n",
        "        dice = self.dice_loss(preds, targets)\n",
        "\n",
        "        return (self.bce_weight * bce) + ((1 - self.bce_weight) * dice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BitWiseMetrics(nn.Module):\n",
        "    def __init__(self, threshold=0.5):\n",
        "        super().__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return bitwise_metric(input, target, self.threshold)\n",
        "\n",
        "def bitwise_metric(preds, trues, threshold=0.5):\n",
        "    preds = (preds > threshold).float()\n",
        "    return (preds == trues).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dice_coefficient(preds, targets, smooth=1e-6):\n",
        "    if preds.max() > 1:\n",
        "        preds = nn.functional.sigmoid(preds)\n",
        "    if targets.max() > 1:\n",
        "        targets = targets / 255.0\n",
        "    \n",
        "    preds = preds.contiguous()\n",
        "    targets = targets.contiguous()\n",
        "\n",
        "    preds = (preds > 0.5).float()\n",
        "    targets = (targets > 0.5).float()\n",
        "\n",
        "    intersection = (preds * targets).sum(dim=(1, 2, 3))\n",
        "    dice = (2. * intersection + smooth) / (preds.sum(dim=(1, 2, 3)) + targets.sum(dim=(1, 2, 3)) + smooth)\n",
        "    \n",
        "    return dice.mean().item()\n",
        "\n",
        "def iou_metric(preds, targets, smooth=1e-6):\n",
        "    if preds.max() > 1:\n",
        "        preds = nn.functional.sigmoid(preds)\n",
        "    if targets.max() > 1:\n",
        "        targets = targets / 255.0\n",
        "\n",
        "    preds = preds.contiguous()\n",
        "    targets = targets.contiguous()\n",
        "    \n",
        "    intersection = (preds * targets).sum(dim=(1, 2, 3))\n",
        "    union = preds.sum(dim=(1, 2, 3)) + targets.sum(dim=(1, 2, 3)) - intersection\n",
        "    iou = (intersection + smooth) / (union + smooth)\n",
        "    \n",
        "    return iou.mean().item()\n",
        "\n",
        "\n",
        "def accuracy_metric(preds, targets):     \n",
        "    if preds.max() > 1:\n",
        "        preds = nn.functional.sigmoid(preds)\n",
        "        \n",
        "    if targets.max() > 1:\n",
        "        targets = targets / 255.0\n",
        "           \n",
        "    preds = preds.contiguous()\n",
        "    targets = targets.contiguous()\n",
        "    \n",
        "    preds = (preds > 0.5).float()\n",
        "    correct = (preds == targets).float()\n",
        "    \n",
        "    return correct.sum() / correct.numel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TRAIN 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, opt, criterion, metrics, device):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_scores = {metric_name: 0.0 for metric_name in metrics.keys()}\n",
        "\n",
        "    for image, mask in train_loader:\n",
        "        image, mask = image.to(device), mask.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        out = model(image)\n",
        "        #out = nn.functional.sigmoid(out)\n",
        "        #if mask.max() > 1:\n",
        "        #    mask = mask / 255.0\n",
        "        loss = criterion(out.float(), mask.float())\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        for metric_name, metric_func in metrics.items():\n",
        "            train_scores[metric_name] += metric_func(out, mask)\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_train_scores = {metric_name: score / len(train_loader)  for metric_name, score in train_scores.items()}\n",
        "\n",
        "    return avg_train_loss, avg_train_scores\n",
        "\n",
        "def validate_one_epoch(model, val_loader, criterion, metrics, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_scores = {metric_name: 0.0 for metric_name in metrics.keys()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, mask in val_loader:\n",
        "            image, mask = image.to(device), mask.to(device)\n",
        "            out = model(image)\n",
        "\n",
        "            #out = nn.functional.sigmoid(out)\n",
        "            #if mask.max() > 1:\n",
        "            #    mask = mask / 255.0\n",
        "            loss = criterion(out.float(), mask.float())\n",
        "            val_loss += loss.item()\n",
        "            for metric_name, metric_func in metrics.items():\n",
        "                val_scores[metric_name] += metric_func(out, mask)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    avg_val_scores = {metric_name: score / len(val_loader)  for metric_name, score in val_scores.items()}\n",
        "\n",
        "    return avg_val_loss, avg_val_scores\n",
        "\n",
        "def fit(model, epochs, opt, criterion, train_loader, val_loader, metrics=None, monitor_metric='val_loss', device='cuda', patience=10, min_delta=1e-4):\n",
        "    if metrics is None:\n",
        "        metrics = {'iou': iou_metric}\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_metrics = {metric_name: [] for metric_name in metrics.keys()}\n",
        "    val_metrics = {metric_name: [] for metric_name in metrics.keys()}\n",
        "\n",
        "    best_monitor_value = 0\n",
        "    best_model_wts = None\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        avg_train_loss, avg_train_scores = train_one_epoch(model, train_loader, opt, criterion, metrics, device)\n",
        "        avg_val_loss, avg_val_scores = validate_one_epoch(model, val_loader, criterion, metrics, device)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        for metric_name, score in avg_train_scores.items():\n",
        "            train_metrics[metric_name].append(score)\n",
        "        \n",
        "        for metric_name, score in avg_val_scores.items():\n",
        "            val_metrics[metric_name].append(score)\n",
        "        \n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.6f}, Validation Loss: {avg_val_loss:.6f}\")\n",
        "        print(f\"Train Scores: {avg_train_scores}\")\n",
        "        print(f\"Valid Scores: {avg_val_scores}\")\n",
        "\n",
        "        # Early stopping based on the monitored metric\n",
        "        current_monitor_value = avg_val_scores[monitor_metric]\n",
        "        \n",
        "        if current_monitor_value - best_monitor_value > min_delta:\n",
        "            best_monitor_value = current_monitor_value\n",
        "            best_model_wts = model.state_dict()\n",
        "            early_stop_counter = 0\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "            print(f\"{monitor_metric}: {current_monitor_value:.6f} did not improve from {best_monitor_value:.6f} having delta of {current_monitor_value - best_monitor_value:.6f} compared to min_delta of {min_delta:.6f}\")\n",
        "        print(f\"Early stopping counter: {early_stop_counter}/{patience}\")\n",
        "\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    # Load the best model weights\n",
        "    if best_model_wts:\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_metrics': train_metrics,\n",
        "        'val_metrics': val_metrics,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AttentionUNet(in_channels=3, out_channels=1).to(device)\n",
        "model.apply(weights_init)\n",
        "criterion = BCEDiceLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "metrics = {\n",
        "    'dice': dice_coefficient,\n",
        "    'iou': iou_metric,\n",
        "    'accuracy': accuracy_metric\n",
        "}\n",
        "\n",
        "history = fit(\n",
        "    model=model,\n",
        "    epochs=EPOCHS,\n",
        "    opt=optimizer,\n",
        "    criterion=criterion,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    metrics=metrics,\n",
        "    monitor_metric='dice', \n",
        "    device='cuda',\n",
        "    patience=PATIENCE,\n",
        "    min_delta=EPS\n",
        ")\n",
        "\n",
        "t_losses = history['train_losses']\n",
        "v_losses = history['val_losses']\n",
        "t_scores = history['train_metrics']\n",
        "v_scores = history['val_metrics']\n",
        "\n",
        "t_dice_scores = t_scores['dice']\n",
        "v_dice_scores = v_scores['dice']\n",
        "\n",
        "t_iou_scores = t_scores['iou']\n",
        "v_iou_scores = v_scores['iou']\n",
        "\n",
        "t_accuracy_scores = t_scores['accuracy']\n",
        "v_accuracy_scores = v_scores['accuracy']\n",
        "\n",
        "print(\"Training Dice Scores:\", t_dice_scores)\n",
        "print(\"Validation Dice Scores:\", v_dice_scores)\n",
        "print(\"Training IOU Scores:\", t_iou_scores)\n",
        "print(\"Validation IOU Scores:\", v_iou_scores)\n",
        "print(\"Training Accuracy Scores:\", t_accuracy_scores)\n",
        "print(\"Validation Accuracy Scores:\", v_accuracy_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name_criterion = \"BCEDiceLoss\" if isinstance(criterion, BCEDiceLoss) else \"DiceLoss\" if isinstance(criterion, DiceLoss) else \"BCELoss\" if isinstance(criterion, nn.BCEWithLogitsLoss) else \"JaccardLoss\" if isinstance(criterion, JaccardLoss) else \"UnknownLoss\"\n",
        "\n",
        "RESULTS_PATH = RESULTS_PATH + name_criterion +'/'\n",
        "\n",
        "if not os.path.exists(RESULTS_PATH):\n",
        "    os.makedirs(RESULTS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loss\n",
        "plt.figure()\n",
        "plt.semilogy(t_losses, label='Training Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (log scale)')\n",
        "plt.legend()\n",
        "plt.savefig(f'{RESULTS_PATH}training_loss.png')\n",
        "plt.close()\n",
        "\n",
        "# Validation Loss\n",
        "plt.figure()\n",
        "plt.semilogy(v_losses, label='Validation Loss')\n",
        "plt.title('Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (log scale)')\n",
        "plt.legend()\n",
        "plt.savefig(f'{RESULTS_PATH}validation_loss.png')\n",
        "plt.close()\n",
        "\n",
        "# Training Score\n",
        "plt.figure()\n",
        "plt.semilogy(t_dice_scores, label='Training Score')\n",
        "plt.title('Training Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (log scale)')\n",
        "plt.legend()\n",
        "plt.savefig(f'{RESULTS_PATH}training_score.png')\n",
        "plt.close()\n",
        "\n",
        "# Validation Score\n",
        "plt.figure()\n",
        "plt.semilogy(v_dice_scores, label='Validation Score')\n",
        "plt.title('Validation Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (log scale)')\n",
        "plt.legend()\n",
        "plt.savefig(f'{RESULTS_PATH}validation_score.png')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model, image):\n",
        "    model.eval()\n",
        "    image = image.to(device)\n",
        "    out = model(image.float())\n",
        "    #out = nn.functional.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "index = random.randint(0, len(val_dataset)- 1)\n",
        "image, mask = val_dataset[index]\n",
        "\n",
        "print(val_images[10])\n",
        "image = image.unsqueeze(0)\n",
        "mask = mask.unsqueeze(0)\n",
        "\n",
        "# predict the mask\n",
        "predicted_mask = predict(model, image)\n",
        "\n",
        "# plot the image, mask and predicted mask\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(image.squeeze().permute(1, 2, 0))\n",
        "plt.title(\"Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(mask.squeeze(), cmap=\"gray\")\n",
        "plt.title(\"Mask\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(predicted_mask.squeeze().cpu().detach().numpy(), cmap=\"gray\")\n",
        "plt.title(\"Predicted Mask\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPCpZRpYAEVI"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_PATH = \"attentionunet_chckpnt_{name_criterion}.pth.tar\"\n",
        "\n",
        "def save_checkpoint(state, filename=\"unet_chckpnt.pth.tar\"):\n",
        "    torch.save(state, RESULTS_PATH + filename)\n",
        "\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model, optimizer, checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_checkpoint(model, CHECKPOINT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def invert_colors(image):\n",
        "    return 1 - image\n",
        "\n",
        "\n",
        "def enhance_brightness(image, factor=1.2):\n",
        "\n",
        "    bright_mask = image > 0.5\n",
        "\n",
        "    enhanced_image = image.clone()\n",
        "    enhanced_image[bright_mask] = torch.clamp(image[bright_mask] * factor, 0, 1)\n",
        "    return enhanced_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TEST\n",
        "DB_PATH_TEST = os.path.join(DB_PATH, \"test/\")\n",
        "\n",
        "set_image = np.zeros((len(DB_PATH_TEST), 128, 128, 3), dtype = np.uint8)\n",
        "\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "img_channel = 3\n",
        "\n",
        "def preprocess(id_, path = DB_PATH_TEST, img_height = 256, img_width = 256, img_channel = 3):\n",
        "    \n",
        "    # initialize two empty array to store\n",
        "    # size is (# of training instance, img_size, img_size, img_channel)\n",
        "    set_image = np.zeros((len(id_), img_height, img_width, img_channel), dtype = np.uint8)\n",
        "    \n",
        "    # iterate through all the training img, save each training instance into X_train\n",
        "    # using tqdm is good for us to visualize the process\n",
        "    for n, id_ in tqdm.tqdm(enumerate(id_), total = len(id_)):   \n",
        "        cur_path = path + id_\n",
        "        # read in img as array\n",
        "        img = imread(cur_path + '/images/' + id_ + '.png')[:,:,:img_channel]  \n",
        "        # resize data to increase the speed of training\n",
        "        img = resize(img, (img_height, img_width), mode='constant', preserve_range=True)\n",
        "        # save current img into X_train\n",
        "        set_image[n] = img  \n",
        "        # for each img, we have several masks\n",
        "        # we need to iterate through each one \n",
        "        \n",
        "    return set_image\n",
        "\n",
        "X_Test = preprocess(os.listdir(DB_PATH_TEST))\n",
        "len(X_Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def predict(model, image):\n",
        "    model.eval()\n",
        "    image = image.to(device)\n",
        "    out = model(image.float())\n",
        "    #out = (out - out.min()) / (out.max() - out.min())\n",
        "    out = nn.functional.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "TEST_PATH = RESULTS_PATH  + \"test/\"\n",
        "\n",
        "if not os.path.exists(TEST_PATH):\n",
        "    os.makedirs(TEST_PATH)\n",
        "\n",
        "# save predicted masks only in a folder called prediction, with the name of the image\n",
        "\n",
        "PREDICT_PATH = TEST_PATH + \"prediction/\"\n",
        "\n",
        "if not os.path.exists(PREDICT_PATH):\n",
        "    os.makedirs(PREDICT_PATH)\n",
        "\n",
        "\n",
        "for index in range(0, len(X_Test)):\n",
        "    image = X_Test[index]\n",
        "    image = TF.to_tensor(image)\n",
        "    image = image.unsqueeze(0)\n",
        "    predicted_mask = predict(model, image)\n",
        "\n",
        "    cv2.imwrite(f'{PREDICT_PATH}{index}.png', predicted_mask.squeeze().cpu().detach().numpy())\n",
        "\n",
        "    plt.figure(figsize=(20, 3))\n",
        "\n",
        "    # Plot the original image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image.squeeze().permute(1, 2, 0).cpu().detach().numpy())\n",
        "    plt.title(\"Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Plot the predicted mask\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(predicted_mask.squeeze().cpu().detach().numpy(), cmap=\"gray\")\n",
        "    plt.title(\"Predicted Mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "\n",
        "    # Save the figure\n",
        "    plt.savefig(f'{TEST_PATH}{index}_test_comparison.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DB_PATH_TEST = os.path.join(DB_PATH, \"test2/\")\n",
        "\n",
        "set_image = np.zeros((len(DB_PATH_TEST), 256, 256, 3), dtype = np.uint8)\n",
        "\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "img_channel = 3\n",
        "\n",
        "def preprocess(ids, path=DB_PATH_TEST, img_height=256, img_width=256, img_channel=3):\n",
        "    # Initialize an empty array to store the images\n",
        "    set_image = np.zeros((len(ids), img_height, img_width, img_channel), dtype=np.uint8)\n",
        "    \n",
        "    # Initialize an index for valid images\n",
        "    valid_image_index = 0\n",
        "    \n",
        "    for n, id_ in tqdm.tqdm(enumerate(ids), total=len(ids)):\n",
        "        try:\n",
        "            cur_path = os.path.join(path, id_)\n",
        "            img = imread(os.path.join(cur_path, 'images', id_ + '.png'))[:, :, :img_channel]\n",
        "            img = resize(img, (img_height, img_width), mode='constant', preserve_range=True)\n",
        "            set_image[valid_image_index] = img\n",
        "            valid_image_index += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {id_}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Truncate the array to only include the valid images\n",
        "    set_image = set_image[:valid_image_index]\n",
        "    \n",
        "    return set_image\n",
        "\n",
        "X_Test = preprocess(os.listdir(DB_PATH_TEST))\n",
        "print(len(X_Test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model, image):\n",
        "    model.eval()\n",
        "    image = image.to(device)\n",
        "    out = model(image.float())\n",
        "    #out = (out - out.min()) / (out.max() - out.min())\n",
        "    out = nn.functional.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "TEST_PATH = RESULTS_PATH  + \"test2/\"\n",
        "\n",
        "if not os.path.exists(TEST_PATH):\n",
        "    os.makedirs(TEST_PATH)\n",
        "\n",
        "PREDICT_PATH = TEST_PATH + \"prediction2/\"\n",
        "\n",
        "if not os.path.exists(PREDICT_PATH):\n",
        "    os.makedirs(PREDICT_PATH)\n",
        "\n",
        "\n",
        "for index in range(0, len(X_Test)):\n",
        "    image = X_Test[index]\n",
        "    image = TF.to_tensor(image)\n",
        "    image = image.unsqueeze(0)\n",
        "    predicted_mask = predict(model, image)\n",
        "    \n",
        "    cv2.imwrite(f'{PREDICT_PATH}{index}.png', predicted_mask.squeeze().cpu().detach().numpy())\n",
        "\n",
        "    plt.figure(figsize=(20, 3))\n",
        "\n",
        "    # Plot the original image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image.squeeze().permute(1, 2, 0).cpu().detach().numpy())\n",
        "    plt.title(\"Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Plot the predicted mask\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(predicted_mask.squeeze().cpu().detach().numpy(), cmap=\"gray\")\n",
        "    plt.title(\"Predicted Mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "\n",
        "    # Save the figure\n",
        "    plt.savefig(f'{TEST_PATH}{index}_test_comparison.png')\n",
        "    plt.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
