{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SydanJainen/Cell-image-segmentation/blob/main/image_Segmantation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUnkC-z2j3sP"
      },
      "source": [
        "# CELL IMAGE SEGMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGnmA8UWy5tw"
      },
      "source": [
        "## HYPERPARAMETER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xEiCoWKNjyc5"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNEL = 3\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 15\n",
        "PADDING = 1\n",
        "STRIDE = 1\n",
        "\n",
        "VAL_SPLIT = 0.3\n",
        "SEED = 24\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "EPS = 1e-3\n",
        "CHECKPOINT_PATH = 'unet_chckpnt.pth.tar'\n",
        "\n",
        "width_out = 128\n",
        "height_out = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbSiAvaAlH6i"
      },
      "source": [
        "## CHECK IF COLAB OR OFFLINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T_aSkDT7lM5u"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  isColab = True\n",
        "else:\n",
        "  isColab = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9csuVQSleyd"
      },
      "source": [
        "## CHECK IF GPU IS AVAILABLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb4Ry-c0lhc4",
        "outputId": "e90183e9-dda3-45d4-ff0f-1bde3f05743c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 GPU available\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "num_gpu = torch.cuda.device_count()\n",
        "print(f'{num_gpu} GPU available')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7_1mjbClJpv"
      },
      "source": [
        "## REQUIREMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r4T96G8lGmp",
        "outputId": "2e272334-3a4f-4114-fdd2-96e455e7c9b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Check if folder is correct\n"
          ]
        }
      ],
      "source": [
        "if isColab:\n",
        "  import os\n",
        "  from google.colab import files, drive\n",
        "  if (not 'kaggle.json' in os.listdir()):\n",
        "    print(\"Upload kaggle.json\")\n",
        "    files.upload()\n",
        "    !mkdir ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "  drive.mount('/content/drive')\n",
        "  DB_PATH = '/content/drive/MyDrive/scuola/deeplife/'\n",
        "else:\n",
        "  print(\"Check if folder is correct\")\n",
        "  DB_PATH = 'database/extract/stage1_train/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3gVoRZVkj1i"
      },
      "source": [
        "## IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "if isColab:\n",
        "    !pip install opencv-python\n",
        "    !pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7uOPomQDkoSl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import tqdm\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "from cv2 import imwrite\n",
        "from skimage.transform import resize\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.io import imread, imshow, imsave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random.seed = SEED\n",
        "np.random.seed = SEED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a9s0UE9y5t2",
        "outputId": "edff37fe-410b-4e45-a559-276d311aa79e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database folder found\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(DB_PATH):\n",
        "    print(\"Database folder found\")\n",
        "else:\n",
        "    print(\" Generate the database\")\n",
        "    os.makedirs(DB_PATH)\n",
        "    os.chdir(DB_PATH)\n",
        "    if(isColab):\n",
        "        !pip install kaggle\n",
        "        !kaggle competitions download -c data-science-bowl-2018\n",
        "        !unzip -q data-science-bowl-2018.zip -d ./\n",
        "        !unzip -q cellular-segmentation/stage1_train.zip -d unpreocessed/\n",
        "        if os.path.exists('./cellular-segmentation') and os.path.exists('./data-science-bowl-2018.zip'):\n",
        "            !rm data-science-bowl-2018.zip\n",
        "    else:\n",
        "        print(\"Download the database and place it in the folder\")\n",
        "    os.chdir('..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZJGiJdHlAgs"
      },
      "source": [
        "## LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6tChQpKompII"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train images: 670\n"
          ]
        }
      ],
      "source": [
        "train_ids = next(os.walk(DB_PATH))[1]\n",
        "# test_ids = next(os.walk(TEST_PATH))[1]\n",
        "\n",
        "print(f'Train images: {len(train_ids)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting and resizing train images and masks ... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 6/670 [00:02<04:02,  2.74it/s]"
          ]
        }
      ],
      "source": [
        "# Get and resize train images and masks\n",
        "x_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL), dtype=np.uint8)\n",
        "y_train = np.zeros((len(train_ids), height_out, width_out, 1))\n",
        "print('Getting and resizing train images and masks ... ')\n",
        "sys.stdout.flush()\n",
        "for n, id_ in tqdm.tqdm(enumerate(train_ids), total=len(train_ids)):\n",
        "    path = DB_PATH + id_\n",
        "    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNEL]\n",
        "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
        "    x_train[n] = img\n",
        "    mask = np.zeros((height_out, width_out, 1))\n",
        "    for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
        "        mask_ = imread(path + '/masks/' + mask_file)\n",
        "        mask_ = np.expand_dims(resize(mask_, (height_out, width_out), mode='constant', \n",
        "                                      preserve_range=True), axis=-1)\n",
        "        mask = np.maximum(mask, mask_)\n",
        "    # save the mask \n",
        "    \n",
        "    y_train[n] = mask\n",
        "\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IMAGE TRAIN PREPROCESS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoOFHafCpXLt"
      },
      "source": [
        "## DEFINE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def overlay_masks(images_dir, subdir_name, target_dir):\n",
        "    train_dir = os.path.join(images_dir, subdir_name)\n",
        "    for mask_dirname in tqdm(glob.glob('{}/*/masks'.format(train_dir))):\n",
        "        masks = []\n",
        "        for image_filepath in glob.glob('{}/*'.format(mask_dirname)):\n",
        "            image = np.asarray(Image.open(image_filepath))\n",
        "            image = np.where(image > 0, 1, 0)\n",
        "            masks.append(image)\n",
        "        overlayed_masks = np.sum(masks, axis=0)\n",
        "        overlayed_masks = np.where(overlayed_masks > 0, 1, 0)\n",
        "        target_filepath = '/'.join(mask_dirname.replace(images_dir, target_dir).split('/')[:-1]) + '.png'\n",
        "        os.makedirs(os.path.dirname(target_filepath), exist_ok=True)\n",
        "        imwrite(target_filepath, overlayed_masks)\n",
        "\n",
        "def preprocess_image(img, target_size=(128, 128)):\n",
        "    img = resize(img, target_size, mode='constant')\n",
        "    x = np.expand_dims(img, axis=0)\n",
        "    x = x.transpose(0, 3, 1, 2)\n",
        "    x = torch.FloatTensor(x)\n",
        "    if torch.cuda.is_available():\n",
        "        x = torch.autograd.Variable(x, volatile=True).cuda()\n",
        "    else:\n",
        "        x = torch.autograd.Variable(x, volatile=True)\n",
        "    return x\n",
        "\n",
        "def save_target_masks(target_filepath, *masks):\n",
        "    with open(target_filepath, 'w') as file:\n",
        "        json.dump(masks, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqLOQRxwnbIU"
      },
      "outputs": [],
      "source": [
        "class CellularDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (string): Directory with all the images.\n",
        "            label_dir (string): Directory with all the labels.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        self.image_files = images\n",
        "        self.label_files = labels\n",
        "\n",
        "        # Ensure both directories have the same number of files\n",
        "        assert len(self.image_files) == len(self.label_files), \\\n",
        "            \"The number of images and labels should be the same\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Construct file paths\n",
        "        img_name = os.path.join(IMAGE_PATH, self.image_files[idx])\n",
        "        label_name = os.path.join(LABEL_PATH, self.label_files[idx])\n",
        "\n",
        "        # Load image and label\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = Image.open(label_name).convert('L')\n",
        "        \n",
        "        # Convert to NumPy arrays\n",
        "        image_np = np.array(image)\n",
        "        label_np = np.array(label)\n",
        "\n",
        "        if self.transform:\n",
        "            image_np, label_np = self.transform(image_np, label_np)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        image_tensor = torch.from_numpy(image_np).float()\n",
        "        label_tensor = torch.from_numpy(label_np).long()\n",
        "\n",
        "        return image_tensor, label_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEFyexFx0aiD"
      },
      "source": [
        "## Create Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform = F.Compose([\n",
        "    F.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
        "    F.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF3Yxii0qGqs"
      },
      "outputs": [],
      "source": [
        "train_dataset = CellularDataset(train_images,train_labels)\n",
        "val_dataset = CellularDataset(val_images,val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6qGkhLA0iin"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FtlKMGZ3FTU"
      },
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM_FQJRV3GXj"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
        "    ):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WRJPsuHnhTI"
      },
      "source": [
        "METRICS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq9xXGOupSTS"
      },
      "source": [
        "## Jaccard Index: Measuring Intersection Over Union (IoU)\n",
        "\n",
        "The **Jaccard Index** is a metric used to compare the similarity between two sets. It calculates the ratio of the size of the intersection of the sets to the size of their union. Mathematically, it's defined as:\n",
        "\n",
        "$$\n",
        "J(A, B) = |A ∩ B| / |A ∪ B|\n",
        "$$\n",
        "\n",
        "## Jaccard Loss \n",
        "\n",
        "The **Jaccard Loss** is derived from the Jaccard index and serves as a loss function during model training. This loss function is particularly useful for image segmentation tasks where the goal is to optimize the model for accurate segmentation. It's defined as:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{Jaccard}}(\\mathbf{p}, \\mathbf{t}) = 1 - J(\\mathbf{p}, \\mathbf{t})\n",
        "$$\n",
        "\n",
        "where $J(\\mathbf{p}, \\mathbf{t})$ is the Jaccard index computed between the predicted and true values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEEGWzztni9P"
      },
      "outputs": [],
      "source": [
        "def jaccard(preds, trues, is_average=True):\n",
        "    num = preds.size(0)\n",
        "    preds = preds.view(num, -1)\n",
        "    trues = trues.view(num, -1)\n",
        "\n",
        "    intersection = (preds * trues).sum(1)\n",
        "    scores = (intersection + EPS) / ((preds + trues).sum(1) - intersection + EPS)\n",
        "\n",
        "    score = scores.sum()\n",
        "    if is_average:\n",
        "        score /= num\n",
        "    return torch.clamp(score, 0., 1.)\n",
        "\n",
        "def jaccard_round(preds, trues, is_average=True):\n",
        "    preds = torch.round(preds)\n",
        "    return jaccard(preds, trues, is_average=is_average)\n",
        "\n",
        "\n",
        "class JaccardLoss(nn.Module):\n",
        "    def __init__(self, size_average=True):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return jaccard(input, target, self.size_average)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dice Loss\n",
        "\n",
        "The Dice Loss is a metric used to evaluate the performance of models in tasks like image segmentation. It focuses on the overlap between the predicted segmentation mask and the ground truth mask, penalizing models that incorrectly classify pixels.\n",
        "\n",
        "A: Predicted segmentation mask pixels\n",
        "\n",
        "B: Ground truth segmentation mask pixels\n",
        "\n",
        "$$\n",
        "L_Dice(A, B) = 1 - 2 * |A ∩ B| / (|A| + |B|)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GoxOigbn0l9"
      },
      "outputs": [],
      "source": [
        "def dice_loss(preds, trues, weight=None, is_average=True):\n",
        "    preds = preds.contiguous()\n",
        "    trues = trues.contiguous()\n",
        "    num = preds.size(0)\n",
        "    preds = preds.view(num, -1)\n",
        "    trues = trues.view(num, -1)\n",
        "    if weight is not None:\n",
        "        w = torch.autograd.Variable(weight).view(num, -1)\n",
        "        preds = preds * w\n",
        "        trues = trues * w\n",
        "    intersection = (preds * trues).sum(1)\n",
        "    scores = (2. * intersection + EPS) / (preds.sum(1) + trues.sum(1) + EPS)\n",
        "\n",
        "    if is_average:\n",
        "        score = scores.sum()/num\n",
        "        return torch.clamp(score, 0., 1.)\n",
        "    else:\n",
        "        return scores\n",
        "\n",
        "def dice_round(preds, trues, is_average=True):\n",
        "    preds = torch.round(preds)\n",
        "    return dice_loss(preds, trues, is_average=is_average)\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "        self.register_buffer('weight', weight)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return dice_loss(input, target, self.weight, self.size_average)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q2uNn-Q32je"
      },
      "source": [
        "## TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPCpZRpYAEVI"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"unet_chckpnt.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model, optimizer, checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2JivtMb3yag"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "accuracy_list = []\n",
        "\n",
        "def train_fn(loader, model, optimizer, loss_fn):\n",
        "    loop = tqdm(loader)\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.permute(0, 3, 1, 2).to(device)  # Transpose to [batch_size, channels, height, width]\n",
        "        targets = targets.float().unsqueeze(1).to(device)\n",
        "\n",
        "        # forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "            train_loss.append(loss)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update tqdm loop\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "\n",
        "def check_accuracy(loader, model, loss_fn, device=\"cuda\"):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.permute(0, 3, 1, 2).to(device)\n",
        "            y = y.to(device).unsqueeze(1)\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "            dice_score += (2 * (preds * y).sum()) / (\n",
        "                (preds + y).sum() + 1e-8\n",
        "            )\n",
        "            val_loss.append(loss_fn(preds, y))\n",
        "\n",
        "    print(\n",
        "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
        "    )\n",
        "    accuracy_list.append(num_correct/num_pixels*100)\n",
        "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
        "    model.train()\n",
        "\n",
        "def fit(model, optimizer, loss_fn):\n",
        "  for epoch in range(EPOCHS):\n",
        "    train_fn(train_loader, model, optimizer, loss_fn)\n",
        "\n",
        "    # check accuracy\n",
        "    check_accuracy(val_loader, model, loss_fn, device=device)\n",
        "\n",
        "    # save model\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\":optimizer.state_dict(),\n",
        "    }\n",
        "    save_checkpoint(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEUN4g0R5eUv"
      },
      "outputs": [],
      "source": [
        "model = UNET(in_channels=3, out_channels=1).to(device)\n",
        "#loss_fn = nn.BCEWithLogitsLoss()\n",
        "loss_fn = JaccardLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXA4o_B052PK",
        "outputId": "3210c93e-781b-4c51-88e9-a6e31da6aacd"
      },
      "outputs": [],
      "source": [
        "fit(model,optimizer,loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "nRGlf1nRAf_5",
        "outputId": "3882b4cf-f679-4722-a44d-b1a84506a470"
      },
      "outputs": [],
      "source": [
        "train_loss_cpu = [tensor.cpu().detach().numpy() for tensor in train_loss]\n",
        "val_loss_cpu = [tensor.cpu().detach().numpy() for tensor in val_loss]\n",
        "accuracy_list_cpu = [tensor.cpu().detach().numpy() for tensor in accuracy_list]\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_loss_cpu, label='train_loss')\n",
        "plt.plot(val_loss_cpu, label='val_loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Plot')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(accuracy_list_cpu, label='accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPGHI9xJf6gg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def to_mask(mask):\n",
        "    mask = mask.clone().cpu().detach().numpy()\n",
        "    mask = mask.transpose((1,2,0))\n",
        "    std = np.array((0.5))\n",
        "    mean = np.array((0.5))\n",
        "    mask  = std * mask + mean\n",
        "    mask = mask.clip(0,1)\n",
        "    mask = np.squeeze(mask)\n",
        "    return mask\n",
        "\n",
        "def to_image(image):\n",
        "    image = image.clone().cpu().numpy()\n",
        "    image = image.transpose((1,2,0))\n",
        "    std = np.array((0.5,0.5,0.5))\n",
        "    mean = np.array((0.5,0.5,0.5))\n",
        "    image  = std * image + mean\n",
        "    image = image.clip(0,1)\n",
        "    image = (image * 255).astype(np.uint8)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the model\n",
        "model, optimizer, start_epoch = load_ckp(CHECKPOINT_PATH, model, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iter_ = iter(val_loader)\n",
        "image,mask = next(iter_)\n",
        "image = image.to(device,dtype=torch.float)\n",
        "mask = mask.to(device,dtype=torch.float)\n",
        "\n",
        "model.eval()\n",
        "# plot the image and mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qKic85Tffw8G",
        "outputId": "fbdd4f6f-62a8-410f-ca3a-fff3b3166f9d"
      },
      "outputs": [],
      "source": [
        "image =  image.permute(0, 3, 1, 2).to(device)\n",
        "# y_pred = model(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
