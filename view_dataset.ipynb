{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project aim\n",
    "\n",
    "The aim of the project is to assess the performance of an UNet or any other Deep model for cell segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorchcuda (Python 3.12.2)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n pytorchcuda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!conda install -n pytorchcuda ipykernel --update-deps --force-reinstall\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, utils\n",
    "from torch import nn\n",
    "\n",
    "import cv2\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn \n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn.functional as F\n",
    "import zipfile\n",
    "\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(img):\n",
    "  \"\"\"Applies image transformations.\n",
    "\n",
    "  Args:\n",
    "      img (PIL Image): The input image to be transformed.\n",
    "\n",
    "  Returns:\n",
    "      torch.Tensor: The transformed image as a PyTorch tensor.\n",
    "  \"\"\"\n",
    "\n",
    "  # Ensure input is a PIL Image\n",
    "  if not isinstance(img, transforms.Image):\n",
    "    raise ValueError(\"Input must be a PIL Image object.\")\n",
    "\n",
    "  # Create a PyTorch transform composed of resize, normalize, and ToTensor\n",
    "  transform = transforms.Compose([\n",
    "      transforms.Resize(256),  # Resize to 256x256\n",
    "      transforms.ToTensor(),   # Convert to PyTorch tensor\n",
    "      transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "  ])\n",
    "\n",
    "  # Apply the transformation to the image\n",
    "  transformed_img = transform(img)\n",
    "\n",
    "  return transformed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataSet(Dataset):\n",
    "        def __init__(self,path, transform=None):\n",
    "            self.path = path\n",
    "            self.folders = os.listdir(path)\n",
    "            self.transforms = apply_transform()\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.folders)\n",
    "              \n",
    "        \n",
    "        def __getitem__(self,idx):\n",
    "            image_folder = os.path.join(self.path,self.folders[idx],'images/')\n",
    "            mask_folder = os.path.join(self.path,self.folders[idx],'masks/')\n",
    "            image_path = os.path.join(image_folder,os.listdir(image_folder)[0])\n",
    "            \n",
    "            img = io.imread(image_path)[:,:,:3].astype('float32')\n",
    "            img = transform.resize(img,(128,128))\n",
    "            \n",
    "            mask = self.get_mask(mask_folder, 128, 128 ).astype('float32')\n",
    "\n",
    "            augmented = self.transforms(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            mask = mask[0].permute(2, 0, 1)\n",
    "            return (img,mask) \n",
    "\n",
    "\n",
    "        def get_mask(self,mask_folder,IMG_HEIGHT, IMG_WIDTH):\n",
    "            mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "            for mask_ in os.listdir(mask_folder):\n",
    "                    mask_ = io.imread(os.path.join(mask_folder,mask_))\n",
    "                    mask_ = transform.resize(mask_, (IMG_HEIGHT, IMG_WIDTH))\n",
    "                    mask_ = np.expand_dims(mask_,axis=-1)\n",
    "                    mask = np.maximum(mask, mask_)\n",
    "              \n",
    "            return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Contraction path\n",
    "        self.c1 = self._contracting_block(in_channels, 16)\n",
    "        self.p1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.c2 = self._contracting_block(16, 32)\n",
    "        self.p2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.c3 = self._contracting_block(32, 64)\n",
    "        self.p3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.c4 = self._contracting_block(64, 128)\n",
    "        self.p4 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.c5 = self._contracting_block(128, 256)\n",
    "\n",
    "        # Expansive path\n",
    "        self.u6 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.c6 = self._expanding_block(256, 128)\n",
    "        self.u7 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.c7 = self._expanding_block(128, 64)\n",
    "        self.u8 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.c8 = self._expanding_block(64, 32)\n",
    "        self.u9 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.c9 = self._expanding_block(32, 16)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputs = nn.Conv2d(16, num_classes, kernel_size=1)\n",
    "\n",
    "    def _contracting_block(self, in_filters, out_filters):\n",
    "        \"\"\"\n",
    "        Single contracting block with convolutional layers, ReLU activation,\n",
    "        dropout, and max pooling.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_filters, out_filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.Conv2d(out_filters, out_filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def _expanding_block(self, in_filters, out_filters):\n",
    "        \"\"\"\n",
    "        Single expanding block with transposed convolution, concatenation,\n",
    "        convolutional layers, ReLU activation, and dropout.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_filters, out_filters, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Concatenate(dim=1),\n",
    "            nn.Conv2d(out_filters * 2, out_filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(out_filters, out_filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the U-Net.\n",
    "        \"\"\"\n",
    "        c1 = self.c1(x)\n",
    "        p1 = self.p1(c1)\n",
    "        c2 = self.c2(p1)\n",
    "        p2 = self.p2(c2)\n",
    "        c3 = self.c3(p2)\n",
    "        p3 = self.p3(c3)\n",
    "        c4 = self.c4(p3)\n",
    "        p4 = self.p4(c4)\n",
    "        c5 = self.c5(p4)\n",
    "\n",
    "        u6 = self.u6(c5)\n",
    "        cat6 = torch.cat([u6, c4], dim=1)  # Concatenate along channel dimension (dim=1)\n",
    "        c6 = self.c6(cat6)\n",
    "        u7 = self.u7(c6)\n",
    "        cat7 = torch.cat([u7, c3], dim=1)\n",
    "        c7 = self.c7(cat7)\n",
    "        u8 = self.u8(c7)\n",
    "        cat8 = torch.cat([u8, c2], dim=1)\n",
    "        c8 = self.c8(cat8)\n",
    "        u9 = self.u9(c8)\n",
    "        cat9 = torch.cat([u9, c1], dim=1)\n",
    "        c9 = self.c9(cat9)\n",
    "\n",
    "        outputs = self.outputs(c9)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "model = UNet(3, 2)  # Input channels=3 (RGB), Output classes=2\n",
    "# x = torch.randn(1, 3, 32, 32)  # Example input tensor\n",
    "# y = model(x)\n",
    "# print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_model_layers(model):\n",
    "  # Iterate through model layers\n",
    "  for i, layer in enumerate(model.modules()):\n",
    "    # Extract layer details\n",
    "    layer_type = type(layer).__name__\n",
    "    input_shape = layer.in_channels, layer.kernel_size[0], layer.kernel_size[1]  # Assuming Conv2d layer\n",
    "    output_shape = layer.out_channels, layer.kernel_size[0], layer.kernel_size[1]  # Assuming Conv2d layer\n",
    "    # ... (extract other relevant properties)\n",
    "\n",
    "    # Create visual representation using Matplotlib/Seaborn\n",
    "    plt.subplot(..., i+1)  # Adjust layout based on number of layers\n",
    "    # ... (code to create visual elements for layer type, shapes, etc.)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "# ... (your model definition)\n",
    "\n",
    "visualize_model_layers(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
